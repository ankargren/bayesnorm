---
output:
  github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# bayesnorm

*Efficient Sampling of Normal Posterior Distributions*

## About

The `bayesnorm` package provides two functions, `rmvn_bcm` and `rmvn_rue`, which 
allow for efficient sampling from normal posterior distributions. The posterior distribution
should have the form `mu = Sigma * Phi' * alpha`, where the posterior
covariance matrix is `Sigma = (Phi' * Phi + D^{-1})^{-1}` and `D` is 
a diagonal matrix. This is the ubiquitous form of normal posteriors. It shows up
in standard Bayesian linear regression as well as when using scale mixtures such 
as the horseshoe prior or other priors from the global-local shrinkage family. The idea
of this package is to provide C++ headers so that sampling routines specifically
tailored for the form of the posterior can be employed, which can speed up computations
notably. The main use of the package is therefore for building and implementing
Gibbs samplers. In addition to the C++ headers, simple wrappers are available for
use from R directly.

## Installation

The package is currently GitHub only and can be installed using `devtools`:
```{r, eval = FALSE}
devtools::install_github("ankargren/bayesnorm")
```

## Sampling routines

The `rmvn_bcm` is appropriate when `n<p`, whereas `rmvn_rue` 
is typically the faster alternative when `n>p`.

The sampling routines are based on the proposals by Bhattacharya, Chakraborty 
and Mallick (2016) and Rue (2001). The former is based on 
an idea of avoiding operations in the `p` dimension in favor of working
in the `n` dimension, which is why it is preferrable when `n<p`. The
latter sampling routine instead does the opposite. The Bhattacharya, Chakraborty 
and Mallick (2016) algorithm has complexity $O(n^2p)$, whereas the Rue (2001) routine
has complexity $O(p^3)$. 

The C++ implementations are available as headers and can therefore be called
directly in C++ (e.g. via Rcpp) if necessary by other packages.

## Example 1

We can first verify that the two functions draw from the correct distribution by
a simple comparison with the sampling function in the `mgcv` package. First generate
some toy data:
```{r, cache = TRUE}
library(bayesnorm)
set.seed(10871)
X <- matrix(rnorm(1000), 50, 20)
d <- runif(20, 0, 1)
alpha <- rnorm(50)
```

Next, call the two functions a large number of times:
```{r, cache = TRUE}
rue <- replicate(100000, rmvn_rue(X, d, alpha), simplify = "matrix")
bcm <- replicate(100000, rmvn_bcm(X, d, alpha), simplify = "matrix")
```

Let's also contrast these results with using an approach where we compute the 
posterior mean and covariance and generate from a normal distribution with
mean `mu` and covariance matrix `Sigma`. The results should be the same, but it
would be an inefficient way to do it.
```{r, cache = TRUE}
Sigma <- solve(crossprod(X) + diag(1/d))
mu <- Sigma %*% crossprod(X, alpha)
mgcv <- mgcv::rmvn(100000,c(mu),Sigma)
```

If we now compute the mean over the draws, we can see that they are close:
```{r}
cbind(rue = rowMeans(rue),
      bcm = rowMeans(bcm),
      mgcv = colMeans(mgcv))
```
While this does not prove anything, it shows us that the means of a large number of
draws using the three functions are essentially the same.

## Example 2

For the second example, we can investigate what use of these two functions mean in terms of efficiency. First, we load the `bayesnorm` and `tidyverse` packages:

```{r}
library(bayesnorm)
library(tidyverse)
```

Next, set the seed and values of `n` and `p` to look at. The function `sample_fun()` computes the posterior mean and covariance and then makes a draw using the `mgcv::rmvn()` function.
```{r}
set.seed(16379)
n_vec <- c(50, 100, 200, 500)
p_vec <- c(50, 100, 200, 500)

sample_fun <- function(X, d, alpha) {
  Sigma <- solve(crossprod(X) + diag(1/d))
  mu <- Sigma %*% crossprod(X, alpha)
  out <- mgcv::rmvn(1,c(mu),Sigma)
  return(out)
}
```

The `data.frame` `combs` stores all the combinations of `n` and `p`; we will go through its rows and fill in timings for the three sampling functions:
```{r}
combs <- expand.grid(n = n_vec, p = p_vec, bcm = NA, mgcv = NA, rue = NA)
```

Finally, loop through `combs`, generate data and time the functions (NB: it takes some time):
```{r, cache = TRUE}
for (i in 1:nrow(combs)) {
  n <- combs[i, 1]
  p <- combs[i, 2]
  
  X <- matrix(rnorm(n*p), n, p)
  d <- runif(p, 0, 1)
  alpha <- rnorm(n)
  mb <- microbenchmark::microbenchmark(rue = rmvn_rue(X, d, alpha),
                                 bcm = rmvn_bcm(X, d, alpha),
                                 mgcv = sample_fun(X, d, alpha), times = 1000)
  
  combs[i, 3:5] <- as_tibble(mb) %>%
    group_by(expr) %>%
    summarize(median = median(time)/1e6) %>%
    arrange(as.character(expr)) %>% 
    pull(median)
  
}
```

Before plotting, put the data into long format:
```{r}
theme_set(theme_minimal())
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", 
               "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
plot_df <- as_tibble(combs) %>%
  gather(bcm:rue, key = "Method", value = "Milliseconds (log scale)")
```

First, we plot the results letting the `x` axis be the number of covariates `p`. For each facet, `n` is fixed. 
```{r}
plot_df %>%
  ggplot(aes(x = p, y = `Milliseconds (log scale)`)) +
  geom_line(aes(color = Method)) +
  facet_grid(n~.,labeller = labeller(.rows = label_both, .cols = label_both)) +
  scale_y_continuous(trans = "log2") +
  scale_color_manual(values = cbPalette)
```

What the figure is showing is that:

1. using the `rmvn_rue()` function, specifically tailored for this specific posterior distribution, is always better than computing `mu` and `Sigma` and then drawing from the posterior
2. `rmvn_rue()` and `mgcv::rmvn()` are unaffected across facets, i.e. the sample size does not affect the computational time
3. if `n` is small relative to `p`, the `rmvn_bcm()` function offers a substantial speed improvement

In the second figure, we instead fix `p` and let the `x` axis be the sample size `n`:
```{r}
plot_df %>%
  ggplot(aes(x = n, y = `Milliseconds (log scale)`)) +
  geom_line(aes(color = Method)) +
  facet_grid(p~.,labeller = labeller(.rows = label_both, .cols = label_both)) +
  scale_y_continuous(trans = "log2") +
  scale_color_manual(values = cbPalette)

```

The Figure tells the same story, but from a different perspective. The take-away message is that notable speed improvements can be obtained by using one of the two sampling routines offered in the package when sampling from normal posterior distributions.

## Incorporation into other packages

### References

Bhattacharya, A., Chakraborty, A. and Mallick, B. (2016) Fast 
sampling with Gaussian scale mixture priors in high-dimensional regression,
*Biometrika*, 103(4):985-991, [doi:10.1093/biomet/asw042](https://doi.org/10.1093/biomet/asw042)

Rue, H. (2001) Fast sampling of Gaussian Markov random fiels, *Journal of the Royal Statistical Society: Series B*, 63, 325-339, [doi:10.1111/1467-9868.00288](https://doi.org/10.1111/1467-9868.00288)